{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords, inaugural\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /home/brandoncsteed/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/brandoncsteed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PlaintextCorpusReader in '/home/brandoncsteed/nltk_data/corpora/inaugural'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for available files\n",
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fellow-Citizens of the Senate and of the House of Representatives:\\n\\nAmong the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable dec'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing first part of raw address text\n",
    "inaugural.raw('1789-Washington.txt')[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(r'-',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "wash1 = inaugural.raw('1789-Washington.txt')\n",
    "jeff1 = inaugural.raw('1801-Jefferson.txt')\n",
    "\n",
    "wash1 = text_cleaner(wash1)\n",
    "jeff1 = text_cleaner(jeff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fellow Citizens of the Senate and of the House of Representatives: Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at cleaned Washington address\n",
    "wash1[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Friends and Fellow Citizens: Called upon to undertake the duties of the first executive office of our country, I avail myself of the presence of that portion of my fellow citizens which is here assemb'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lookign at cleaned Jefferson address\n",
    "jeff1[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the cleaned novels\n",
    "nlp = spacy.load('en')\n",
    "wash1_doc = nlp(wash1)\n",
    "jeff1_doc = nlp(jeff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Fellow, Citizens, of, the, Senate, and, of, t...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Among, the, vicissitudes, incident, to, life,...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(On, the, one, hand, ,, I, was, summoned, by, ...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(On, the, other, hand, ,, the, magnitude, and,...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(In, this, conflict, of, emotions, all, I, dar...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0           1\n",
       "0  (Fellow, Citizens, of, the, Senate, and, of, t...  Washington\n",
       "1  (Among, the, vicissitudes, incident, to, life,...  Washington\n",
       "2  (On, the, one, hand, ,, I, was, summoned, by, ...  Washington\n",
       "3  (On, the, other, hand, ,, the, magnitude, and,...  Washington\n",
       "4  (In, this, conflict, of, emotions, all, I, dar...  Washington"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping into sentences\n",
    "wash1_sents = [[sent, \"Washington\"] for sent in wash1_doc.sents]\n",
    "jeff1_sents = [[sent, \"Jefferson\"] for sent in jeff1_doc.sents]\n",
    "\n",
    "# Combining the sentences from the two novels into one data frame\n",
    "sentences = pd.DataFrame(wash1_sents + jeff1_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "wash1words = bag_of_words(wash1_doc)\n",
    "jeff1words = bag_of_words(jeff1_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(wash1words + jeff1words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asylum</th>\n",
       "      <th>department</th>\n",
       "      <th>character</th>\n",
       "      <th>consultation</th>\n",
       "      <th>preserve</th>\n",
       "      <th>united</th>\n",
       "      <th>year</th>\n",
       "      <th>pursuit</th>\n",
       "      <th>throe</th>\n",
       "      <th>light</th>\n",
       "      <th>...</th>\n",
       "      <th>independent</th>\n",
       "      <th>refer</th>\n",
       "      <th>error</th>\n",
       "      <th>strong</th>\n",
       "      <th>mankind</th>\n",
       "      <th>objection</th>\n",
       "      <th>far</th>\n",
       "      <th>conspicuous</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Fellow, Citizens, of, the, Senate, and, of, t...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Among, the, vicissitudes, incident, to, life,...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(On, the, one, hand, ,, I, was, summoned, by, ...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(On, the, other, hand, ,, the, magnitude, and,...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(In, this, conflict, of, emotions, all, I, dar...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 852 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  asylum department character consultation preserve united year pursuit throe  \\\n",
       "0      0          0         0            0        0      0    0       0     0   \n",
       "1      0          0         0            0        0      0    0       0     0   \n",
       "2      1          0         0            0        0      0    1       0     0   \n",
       "3      0          0         0            0        0      0    0       0     0   \n",
       "4      0          0         0            0        0      0    0       0     0   \n",
       "\n",
       "  light     ...     independent refer error strong mankind objection far  \\\n",
       "0     0     ...               0     0     0      0       0         0   0   \n",
       "1     0     ...               0     0     0      0       0         0   0   \n",
       "2     0     ...               0     0     0      0       0         0   0   \n",
       "3     0     ...               0     0     0      0       0         0   0   \n",
       "4     0     ...               0     0     0      0       0         0   0   \n",
       "\n",
       "  conspicuous                                      text_sentence text_source  \n",
       "0           0  (Fellow, Citizens, of, the, Senate, and, of, t...  Washington  \n",
       "1           0  (Among, the, vicissitudes, incident, to, life,...  Washington  \n",
       "2           0  (On, the, one, hand, ,, I, was, summoned, by, ...  Washington  \n",
       "3           0  (On, the, other, hand, ,, the, magnitude, and,...  Washington  \n",
       "4           0  (In, this, conflict, of, emotions, all, I, dar...  Washington  \n",
       "\n",
       "[5 rows x 852 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.0\n",
      "\n",
      "Test set score: 0.7407407407407407\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 850) (40,)\n",
      "Training set score: 1.0\n",
      "\n",
      "Test set score: 0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.0\n",
      "\n",
      "Test set score: 0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.0\n",
      "\n",
      "Test set score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#Using SVM as a modeling technique\n",
    "svm = SVC(C=250)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', svm.score(X_train, y_train))\n",
    "print('\\nTest set score:', svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8\n",
      "\n",
      "Test set score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "train = bnb.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', bnb.score(X_train, y_train))\n",
    "print('\\nTest set score:', bnb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data, this time in the form of paragraphs\n",
    "was = [[sent, \"Washington\"] for sent in inaugural.paras('1789-Washington.txt')]\n",
    "jef = [[sent, \"Jefferson\"] for sent in inaugural.sents('1801-Jefferson.txt')]\n",
    "#was=inaugural.sents('1789-Washington.txt')\n",
    "#jef = inaugural.sents('1801-Jefferson.txt')\n",
    "inaugural1 = pd.DataFrame(was + jef)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /home/brandoncsteed/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n",
      "['Fellow  Citizens of the Senate and of the House of Representatives :', 'Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order , and received on the 14th day of the present month .', 'On the one hand , I was summoned by my Country , whose voice I can never hear but with veneration and love , from a retreat which I had chosen with the fondest predilection , and , in my flattering hopes , with an immutable decision , as the asylum of my declining years  a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination , and of frequent interruptions in my health to the gradual waste committed on it by time .', 'On the other hand , the magnitude and difficulty of the trust to which the voice of my country called me , being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications , could not but overwhelm with despondence one who ( inheriting inferior endowments from nature and unpracticed in the duties of civil administration ) ought to be peculiarly conscious of his own deficiencies .']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "nltk.download('inaugural')\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#reading in the data, this time in the form of paragraphs\n",
    "wash=inaugural.sents('1789-Washington.txt')\n",
    "#processing\n",
    "wash_sents=[]\n",
    "for sentence in wash:\n",
    "    #removing the double-dash from all words\n",
    "    sentence=[re.sub(r'-','',word) for word in sentence]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    wash_sents.append(' '.join(sentence))\n",
    "\n",
    "print(wash_sents[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Friends and Fellow Citizens :', 'Called upon to undertake the duties of the first executive office of our country , I avail myself of the presence of that portion of my fellow citizens which is here assembled to express my grateful thanks for the favor with which they have been pleased to look toward me , to declare a sincere consciousness that the task is above my talents , and that I approach it with those anxious and awful presentiments which the greatness of the charge and the weakness of my powers so justly inspire .', 'A rising nation , spread over a wide and fruitful land , traversing all the seas with the rich productions of their industry , engaged in commerce with nations who feel power and forget right , advancing rapidly to destinies beyond the reach of mortal eye  when I contemplate these transcendent objects , and see the honor , the happiness , and the hopes of this beloved country committed to the issue , and the auspices of this day , I shrink from the contemplation , and humble myself before the magnitude of the undertaking .', 'Utterly , indeed , should I despair did not the presence of many whom I here see remind me that in the other high authorities provided by our Constitution I shall find resources of wisdom , of virtue , and of zeal on which to rely under all difficulties .']\n"
     ]
    }
   ],
   "source": [
    "#reading in the data, this time in the form of paragraphs\n",
    "jeff=inaugural.sents('1801-Jefferson.txt')\n",
    "#processing\n",
    "jeff_sents=[]\n",
    "for sentence in jeff:\n",
    "    #removing the double-dash from all words\n",
    "    sentence=[re.sub(r'-','',word) for word in sentence]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    jeff_sents.append(' '.join(sentence))\n",
    "\n",
    "print(jeff_sents[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fellow  Citizens of the Senate and of the Hous...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Among the vicissitudes incident to life no eve...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On the one hand , I was summoned by my Country...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On the other hand , the magnitude and difficul...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In this conflict of emotions all I dare aver i...</td>\n",
       "      <td>Washington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0           1\n",
       "0  Fellow  Citizens of the Senate and of the Hous...  Washington\n",
       "1  Among the vicissitudes incident to life no eve...  Washington\n",
       "2  On the one hand , I was summoned by my Country...  Washington\n",
       "3  On the other hand , the magnitude and difficul...  Washington\n",
       "4  In this conflict of emotions all I dare aver i...  Washington"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping into sentences\n",
    "wash2_sents = [[sent, \"Washington\"] for sent in wash_sents]\n",
    "jeff2_sents = [[sent, \"Jefferson\"] for sent in jeff_sents]\n",
    "\n",
    "# Combining the sentences from the two novels into one data frame\n",
    "inaugural_sents = pd.DataFrame(wash2_sents + jeff2_sents)\n",
    "inaugural_sents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Fellow  Citizens of the Senate and of the Hous...\n",
       "1     Among the vicissitudes incident to life no eve...\n",
       "2     On the one hand , I was summoned by my Country...\n",
       "3     On the other hand , the magnitude and difficul...\n",
       "4     In this conflict of emotions all I dare aver i...\n",
       "5     All I dare hope is that if , in executing this...\n",
       "6     Such being the impressions under which I have ...\n",
       "7     In tendering this homage to the Great Author o...\n",
       "8     No people can be bound to acknowledge and ador...\n",
       "9     Every step by which they have advanced to the ...\n",
       "10    These reflections , arising out of the present...\n",
       "11    You will join with me , I trust , in thinking ...\n",
       "12    By the article establishing the executive depa...\n",
       "13    The circumstances under which I now meet you w...\n",
       "14    It will be more consistent with those circumst...\n",
       "15    In these honorable qualifications I behold the...\n",
       "16    I dwell on this prospect with every satisfacti...\n",
       "17    Besides the ordinary objects submitted to your...\n",
       "18    Instead of undertaking particular recommendati...\n",
       "19    To the foregoing observations I have one to ad...\n",
       "20    It concerns myself , and will therefore be as ...\n",
       "21    When I was first honored with a call into the ...\n",
       "22    From this resolution I have in no instance dep...\n",
       "23    Having thus imparted to you my sentiments as t...\n",
       "24                        Friends and Fellow Citizens :\n",
       "25    Called upon to undertake the duties of the fir...\n",
       "26    A rising nation , spread over a wide and fruit...\n",
       "27    Utterly , indeed , should I despair did not th...\n",
       "28    To you , then , gentlemen , who are charged wi...\n",
       "29    During the contest of opinion through which we...\n",
       "                            ...                        \n",
       "36    We have called by different names brethren of ...\n",
       "37    We are all Republicans , we are all Federalists .\n",
       "38    If there be any among us who would wish to dis...\n",
       "39    I know , indeed , that some honest men fear th...\n",
       "40                                        I trust not .\n",
       "41    I believe this , on the contrary , the stronge...\n",
       "42    I believe it the only one where every man , at...\n",
       "43    Sometimes it is said that man can not be trust...\n",
       "44    Can he , then , be trusted with the government...\n",
       "45    Or have we found angels in the forms of kings ...\n",
       "46                   Let history answer this question .\n",
       "47    Let us , then , with courage and confidence pu...\n",
       "48    Kindly separated by nature and a wide ocean fr...\n",
       "49    Still one thing more , fellow citizens  a wise...\n",
       "50    This is the sum of good government , and this ...\n",
       "51    About to enter , fellow  citizens , on the exe...\n",
       "52    I will compress them within the narrowest comp...\n",
       "53    Equal and exact justice to all men , of whatev...\n",
       "54    These principles form the bright constellation...\n",
       "55    The wisdom of our sages and blood of our heroe...\n",
       "56    They should be the creed of our political fait...\n",
       "57    I repair , then , fellow  citizens , to the po...\n",
       "58    With experience enough in subordinate offices ...\n",
       "59    Without pretensions to that high confidence yo...\n",
       "60    I shall often go wrong through defect of judgm...\n",
       "61    When right , I shall often be thought wrong by...\n",
       "62    I ask your indulgence for my own errors , whic...\n",
       "63    The approbation implied by your suffrage is a ...\n",
       "64    Relying , then , on the patronage of your good...\n",
       "65    And may that Infinite Power which rules the de...\n",
       "Name: 0, Length: 66, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 67\n",
      "Original sentence: 56    They should be the creed of our political fait...\n",
      "42    I believe it the only one where every man , at...\n",
      "43    Sometimes it is said that man can not be trust...\n",
      "60    I shall often go wrong through defect of judgm...\n",
      "18    Instead of undertaking particular recommendati...\n",
      "62    I ask your indulgence for my own errors , whic...\n",
      "15    In these honorable qualifications I behold the...\n",
      "5     All I dare hope is that if , in executing this...\n",
      "16    I dwell on this prospect with every satisfacti...\n",
      "20    It concerns myself , and will therefore be as ...\n",
      "57    I repair , then , fellow  citizens , to the po...\n",
      "8     No people can be bound to acknowledge and ador...\n",
      "13    The circumstances under which I now meet you w...\n",
      "25    Called upon to undertake the duties of the fir...\n",
      "37    We are all Republicans , we are all Federalists .\n",
      "17    Besides the ordinary objects submitted to your...\n",
      "49    Still one thing more , fellow citizens  a wise...\n",
      "52    I will compress them within the narrowest comp...\n",
      "58    With experience enough in subordinate offices ...\n",
      "38    If there be any among us who would wish to dis...\n",
      "1     Among the vicissitudes incident to life no eve...\n",
      "12    By the article establishing the executive depa...\n",
      "54    These principles form the bright constellation...\n",
      "24                        Friends and Fellow Citizens :\n",
      "6     Such being the impressions under which I have ...\n",
      "23    Having thus imparted to you my sentiments as t...\n",
      "36    We have called by different names brethren of ...\n",
      "50    This is the sum of good government , and this ...\n",
      "21    When I was first honored with a call into the ...\n",
      "19    To the foregoing observations I have one to ad...\n",
      "9     Every step by which they have advanced to the ...\n",
      "39    I know , indeed , that some honest men fear th...\n",
      "61    When right , I shall often be thought wrong by...\n",
      "59    Without pretensions to that high confidence yo...\n",
      "3     On the other hand , the magnitude and difficul...\n",
      "0     Fellow  Citizens of the Senate and of the Hous...\n",
      "53    Equal and exact justice to all men , of whatev...\n",
      "47    Let us , then , with courage and confidence pu...\n",
      "44    Can he , then , be trusted with the government...\n",
      "Name: 0, dtype: object\n",
      "Tf_idf vector: {'peace': 0.4002603713059492, 'political': 0.4002603713059492, 'let': 0.3276018037352064, 'safety': 0.35775783392095045, 'liberty': 0.37686953707065535, 'error': 0.4002603713059492, 'trust': 0.37686953707065535}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(inaugural_sents, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least three times\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "inaugural_sents_tfidf=vectorizer.fit_transform(inaugural_sents[0])\n",
    "print(\"Number of features: %d\" % inaugural_sents_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(inaugural_sents_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[0])\n",
    "print('Tf_idf vector:', tfidf_bypara[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = inaugural_sents[1]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inaugural_sents_tfidf, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<46x67 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 207 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 24)\t0.3908418767234323\n",
      "  (1, 4)\t0.6986789262497043\n",
      "  (1, 0)\t0.36800145032593373\n",
      "  (1, 31)\t0.36800145032593373\n",
      "  (1, 6)\t0.2970526056608228\n",
      "  (2, 44)\t0.5167068779727263\n",
      "  (2, 1)\t0.17223562599090878\n",
      "  (2, 46)\t0.17223562599090878\n",
      "  (2, 49)\t0.30789280626248505\n",
      "  (2, 58)\t0.15394640313124253\n",
      "  (2, 61)\t0.17223562599090878\n",
      "  (2, 55)\t0.17223562599090878\n",
      "  (2, 57)\t0.17223562599090878\n",
      "  (2, 54)\t0.15394640313124253\n",
      "  (2, 25)\t0.34447125198181755\n",
      "  (2, 10)\t0.16217034032742758\n",
      "  (2, 53)\t0.17223562599090878\n",
      "  (2, 34)\t0.16217034032742758\n",
      "  (2, 20)\t0.10696977891894652\n",
      "  (2, 45)\t0.14699316330628157\n",
      "  (2, 37)\t0.17223562599090878\n",
      "  (2, 51)\t0.42290997900679844\n",
      "  (3, 62)\t1.0\n",
      "  (4, 61)\t1.0\n",
      "  (5, 49)\t0.6664121293958076\n",
      "  :\t:\n",
      "  (44, 27)\t0.3143708714720647\n",
      "  (44, 59)\t0.7719110253160831\n",
      "  (44, 17)\t0.2809887015762525\n",
      "  (44, 34)\t0.2959993376651688\n",
      "  (44, 20)\t0.1952452196022345\n",
      "  (44, 3)\t0.21733154416716483\n",
      "  (44, 15)\t0.23108520070751393\n",
      "  (45, 52)\t0.2449118618970982\n",
      "  (45, 30)\t0.2449118618970982\n",
      "  (45, 57)\t0.2601125937164445\n",
      "  (45, 54)\t0.23249195967098296\n",
      "  (45, 25)\t0.2601125937164445\n",
      "  (45, 63)\t0.2449118618970982\n",
      "  (45, 36)\t0.2449118618970982\n",
      "  (45, 20)\t0.16154722046500697\n",
      "  (45, 45)\t0.22199108196233433\n",
      "  (45, 23)\t0.22199108196233433\n",
      "  (45, 56)\t0.2449118618970982\n",
      "  (45, 51)\t0.21289480794144153\n",
      "  (45, 9)\t0.2449118618970982\n",
      "  (45, 43)\t0.23249195967098296\n",
      "  (45, 38)\t0.2449118618970982\n",
      "  (45, 31)\t0.2449118618970982\n",
      "  (45, 6)\t0.19769407612209527\n",
      "  (45, 42)\t0.2601125937164445\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x67 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15    Washington\n",
       "60     Jefferson\n",
       "23    Washington\n",
       "61     Jefferson\n",
       "12    Washington\n",
       "34     Jefferson\n",
       "0     Washington\n",
       "17    Washington\n",
       "41     Jefferson\n",
       "9     Washington\n",
       "54     Jefferson\n",
       "31     Jefferson\n",
       "28     Jefferson\n",
       "27     Jefferson\n",
       "2     Washington\n",
       "18    Washington\n",
       "35     Jefferson\n",
       "50     Jefferson\n",
       "52     Jefferson\n",
       "38     Jefferson\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9347826086956522\n",
      "\n",
      "Test set score: 0.8\n"
     ]
    }
   ],
   "source": [
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, Y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8695652173913043\n",
      "\n",
      "Test set score: 0.85\n"
     ]
    }
   ],
   "source": [
    "bnb.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score:', bnb.score(X_train, Y_train))\n",
    "print('\\nTest set score:', bnb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9565217391304348\n",
      "\n",
      "Test set score: 0.8\n"
     ]
    }
   ],
   "source": [
    "svm.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score:', svm.score(X_train, Y_train))\n",
    "print('\\nTest set score:', svm.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.782608695652174\n",
      "\n",
      "Test set score: 0.6\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_train, Y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9565217391304348\n",
      "\n",
      "Test set score: 0.85\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, Y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 206\n",
      "\n",
      "Original sentence: \n",
      "\n",
      "Tf_idf vector: {'services': 0.32620100630785664, 'steps': 0.32620100630785664, 'faith': 0.32620100630785664, 'moments': 0.32620100630785664, 'peace': 0.30334656266038423, 'political': 0.30334656266038423, 'let': 0.24828058985748414, 'safety': 0.27113503350495666, 'liberty': 0.2856192789428273, 'error': 0.30334656266038423, 'trust': 0.2856192789428273}\n",
      "\n",
      "Training set score: 0.9565217391304348\n",
      "\n",
      "Test set score: 0.85\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "inaugural_sents_tfidf=vectorizer.fit_transform(inaugural_sents[0])\n",
    "print(\"Number of features: %d\" % inaugural_sents_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(inaugural_sents_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('\\nOriginal sentence:', X_train[0])\n",
    "print('\\nTf_idf vector:', tfidf_bypara[0])\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print('\\nTraining set score:', clf.score(X_train, Y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
